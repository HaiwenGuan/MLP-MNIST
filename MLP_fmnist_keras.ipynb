{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_fmnist_keras.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNT70I1AET9gzWNM6CL+D4Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaiwenGuan/MLP-MNIST/blob/main/MLP_fmnist_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-8FTL9oWE1J",
        "outputId": "00763f35-662f-4592-fb18-617d8bf74af8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from keras.datasets import mnist\n",
        "from keras.datasets import fashion_mnist\n",
        "from google.colab import drive\n",
        "import math\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "np.random.seed(4208)\n",
        "tf.random.set_seed(4208)\n",
        "\n",
        "tf.executing_eagerly()\n",
        "tf.__version__\n",
        "\n",
        "tf.config.list_physical_devices('CPU')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size_input = 784\n",
        "size_hidden_1 = 256\n",
        "size_hidden_2 = 128\n",
        "size_output = 10"
      ],
      "metadata": {
        "id": "lnS9GA45Wkfe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(4208)\n",
        "tf.random.set_seed(4208)"
      ],
      "metadata": {
        "id": "CXOPfN0TYi8D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train),(X_test, y_test) = fashion_mnist.load_data()\n",
        "X_train = tf.cast(tf.reshape(X_train, (-1, 784)), dtype=tf.float32)\n",
        "X_test = tf.cast(tf.reshape(X_test, (-1, 784)), dtype=tf.float32)\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(100)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5z7lPIlWlxa",
        "outputId": "d6b1f5f1-78e9-4446-9e3c-65db8c896095"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden_1, size_hidden_2, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: etiher 'cpu' or 'gpu' or None. If None, decided automatically during eager.\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden_1, self.size_hidden_2, self.size_output, self.device =\\\n",
        "    size_input, size_hidden_1, size_hidden_2, size_output, device\n",
        "\n",
        "    initializer = tf.keras.initializers.HeUniform()\n",
        "\n",
        "    #Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(initializer(shape=(self.size_input,self.size_hidden_1)))\n",
        "    self.b1 = tf.Variable(initializer(shape=(1,self.size_hidden_1)))\n",
        "    self.W2 = tf.Variable(initializer(shape=(self.size_hidden_1, self.size_hidden_2)))\n",
        "    self.b2 = tf.Variable(initializer(shape=(1,self.size_hidden_2)))\n",
        "    self.Wop = tf.Variable(initializer(shape=(self.size_hidden_2, self.size_output)))\n",
        "    self.bop = tf.Variable(initializer(shape=(1,self.size_output)))\n",
        "\n",
        "    self.variables = [self.W1, self.W2, self.Wop, self.b1, self.b2, self.bop]  # define variables(parameters) that will be updated\n",
        "\n",
        "  def forward(self, X): # (X is the input matrix)\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "    \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape(batch_size, size_output)\n",
        "    y_true - Tensor of shape(batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    return scce(y_true, y_pred)\n",
        "\n",
        "  def loss_l1(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape(batch_size, size_output)\n",
        "    y_true - Tensor of shape(batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    penalty = (tf.reduce_sum(tf.math.abs(self.W1)).numpy() + tf.reduce_sum(tf.math.abs(self.W2)).numpy()\\\n",
        "               + tf.reduce_sum(tf.math.abs(self.Wop).numpy())) \\\n",
        "               / ( y_pred.shape[1] )\n",
        "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    return scce(y_true, y_pred) + penalty\n",
        "\n",
        "  def loss_l2(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape(batch_size, size_output)\n",
        "    y_true - Tensor of shape(batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    penalty = (tf.reduce_sum(tf.math.square(self.W1)).numpy() + tf.reduce_sum(tf.math.square(self.W2)).numpy() \\\n",
        "               + tf.reduce_sum(tf.math.square(self.Wop)).numpy())\\\n",
        "               / ( y_pred.shape[1] )\n",
        "    #penalty = self.l2 * (tf.reduce_sum(tf.math.square(self.W1)).numpy() + tf.reduce_sum(tf.math.square(self.W2)).numpy() + tf.reduce_sum(tf.math.square(self.Wop).numpy()))\n",
        "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    return scce(y_true, y_pred) + penalty\n",
        "\n",
        "  def backward(self, X_train, y_train):\n",
        "    '''\n",
        "    backward pass\n",
        "    '''\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch(self.variables)\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients( zip( grads, self.variables))\n",
        "\n",
        "  def backward_1(self, X_train, y_train):\n",
        "    '''\n",
        "    backward pass\n",
        "    '''\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-3)\n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch(self.variables)\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss_l1(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients( zip( grads, self.variables))\n",
        "\n",
        "  def backward_2(self, X_train, y_train):\n",
        "    '''\n",
        "    backward pass\n",
        "    '''\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    with tf.GradientTape() as tape:\n",
        "      tape.watch(self.variables)\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss_l2(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients( zip( grads, self.variables))\n",
        "    \n",
        "  def compute_output(self, X):\n",
        "#    X_tf = tf.cast(X, dytpe=tf.float32)\n",
        "    what1 = tf.matmul(X, self.W1) + self.b1\n",
        "    hhat1 = tf.keras.activations.relu(what1)\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.keras.activations.relu(what2)\n",
        "#    output = tf.nn.softmax(tf.matmul(hhat2, self.Wop) + self.bop)\n",
        "    output = tf.matmul(hhat2, self.Wop) + self.bop\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "CxdpfaMhWrhp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "r4RljWbxYcU-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_on_gpu = MLP(size_input, size_hidden_1, size_hidden_2, size_output, device='gpu')\n",
        "m = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1],dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy = []\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(100, seed=epoch*(4208)).batch(100)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_gpu.loss_l1(preds, outputs)\n",
        "    #lt = lt + mlp_on_gpu.loss(outputs, preds)\n",
        "    m.update_state(outputs,preds)\n",
        "    accuracy.append(m.result().numpy())\n",
        "    mlp_on_gpu.backward_1(inputs, outputs)\n",
        "  #print('Number of Epoch = {} - Average Loss:= {}'.format(epoch + 1, np.sum(loss_total) / 600))\n",
        "  m.reset_state()\n",
        "  print('Number of Epoch = {} - Accuracy:= {} - Average Loss:= {}'.format(epoch + 1, np.sum(accuracy)/len(accuracy), np.sum(loss_total)/X_train.shape[0]))\n",
        "time_taken = time.time() - time_start\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG1c1XdCXFL3",
        "outputId": "cb2da98e-f294-46ff-d380-9d4ed5bd0391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Epoch = 1 - Accuracy:= 0.3348852030436198 - Average Loss:= 0.017916996256510418\n",
            "Number of Epoch = 2 - Accuracy:= 0.6675503540039063 - Average Loss:= 0.011633783976236978\n",
            "Number of Epoch = 3 - Accuracy:= 0.7160539754231771 - Average Loss:= 0.0095250244140625\n",
            "Number of Epoch = 4 - Accuracy:= 0.7362216186523437 - Average Loss:= 0.008537568155924479\n",
            "Number of Epoch = 5 - Accuracy:= 0.749415995279948 - Average Loss:= 0.007952610270182292\n",
            "Number of Epoch = 6 - Accuracy:= 0.7594038899739584 - Average Loss:= 0.007555587768554688\n",
            "Number of Epoch = 7 - Accuracy:= 0.7676725260416667 - Average Loss:= 0.007261663818359375\n",
            "Number of Epoch = 8 - Accuracy:= 0.7745119222005208 - Average Loss:= 0.0070304931640625\n",
            "Number of Epoch = 9 - Accuracy:= 0.7793938191731771 - Average Loss:= 0.006841766357421875\n",
            "Number of Epoch = 10 - Accuracy:= 0.7849311319986979 - Average Loss:= 0.0066831466674804685\n",
            "\n",
            "Total time taken (in seconds): 192.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_total = tf.Variable(0,dtype=tf.float32)\n",
        "accuracy = []\n",
        "m = tf.keras.metrics.CategoricalAccuracy()\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_gpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_gpu.loss_l1(preds, outputs)\n",
        "  m.update_state(outputs,preds)\n",
        "  accuracy.append(m.result().numpy())\n",
        "#print('Test MSE: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_train.shape[0]))\n",
        "print('Accuracy: {:.4f} - Loss: {:.4f}'.format(np.sum(accuracy) / len(accuracy), np.sum(test_loss_total.numpy()) / X_train.shape[0]))"
      ],
      "metadata": {
        "id": "vNzJ8f5adU7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 4208\n",
        "#f = open(\"/content/drive/My Drive/Colab Notebooks/MLP/FMNIST_adam_noreg.txt\",\"w\")\n",
        "lines = []\n",
        "time_start = time.time()\n",
        "for i in range(10):\n",
        "  print(\"==============\" + str(i)+\"=============\")\n",
        "  seed = seed + i\n",
        "\n",
        "  mlp_on_gpu = MLP(size_input, size_hidden_1, size_hidden_2, size_output, device='cpu')\n",
        "  m = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loss_total = tf.zeros([1,1],dtype=tf.float32)\n",
        "    lt = 0\n",
        "    accuracy = []\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(50, seed=epoch*(seed)).batch(100)\n",
        "    for inputs, outputs in train_ds:\n",
        "      preds = mlp_on_gpu.forward(inputs)\n",
        "      loss_total = loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "      lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "      m.update_state(outputs,preds)\n",
        "      accuracy.append(m.result().numpy())\n",
        "      mlp_on_gpu.backward(inputs, outputs)\n",
        "    #print('Number of Epoch = {} - Average Loss:= {}'.format(epoch + 1, np.sum(loss_total) / 600))\n",
        "    m.reset_state()\n",
        "    print('Number of Epoch = {} - Accuracy:= {} - Average Loss:= {}\\n'.format(epoch + 1, np.sum(accuracy)/len(accuracy), np.sum(loss_total)/X_train.shape[0]))\n",
        "    lines.append('Number of Epoch = {} - Accuracy:= {} - Average Loss:= {}\\n'.format(epoch + 1, np.sum(accuracy)/len(accuracy), np.sum(loss_total)/X_train.shape[0]))\n",
        "  test_loss_total = tf.Variable(0,dtype=tf.float32)\n",
        "  accuracy = []\n",
        "  n = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "  for inputs, outputs in test_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "    n.update_state(outputs,preds)\n",
        "    accuracy.append(n.result().numpy())\n",
        "#print('Test MSE: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_train.shape[0]))\n",
        "  lines.append('Test Accuracy: {:.4f} - Loss: {:.4f}\\n'.format(np.sum(accuracy) / len(accuracy), np.sum(test_loss_total.numpy()) / X_train.shape[0]))\n",
        "  print('Test Accuracy: {:.4f} - Loss: {:.4f}\\n'.format(np.sum(accuracy) / len(accuracy), np.sum(test_loss_total.numpy()) / X_train.shape[0]))\n",
        "time_taken = time.time() - time_start\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS6srBO1dj0K",
        "outputId": "126167f1-5273-4ed3-9e2a-2ee0996257cc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============0=============\n",
            "Number of Epoch = 1 - Accuracy:= 0.7053968302408854 - Average Loss:= 0.006436711629231771\n",
            "\n",
            "Number of Epoch = 2 - Accuracy:= 0.821185048421224 - Average Loss:= 0.005109065246582031\n",
            "\n",
            "Number of Epoch = 3 - Accuracy:= 0.8362662760416667 - Average Loss:= 0.005213197835286458\n",
            "\n",
            "Number of Epoch = 4 - Accuracy:= 0.8383838399251302 - Average Loss:= 0.0056189921061197915\n",
            "\n",
            "Number of Epoch = 5 - Accuracy:= 0.8439528401692709 - Average Loss:= 0.005891758219401042\n",
            "\n",
            "Number of Epoch = 6 - Accuracy:= 0.8478350830078125 - Average Loss:= 0.0062336090087890626\n",
            "\n",
            "Number of Epoch = 7 - Accuracy:= 0.8478074137369792 - Average Loss:= 0.006331155904134115\n",
            "\n",
            "Number of Epoch = 8 - Accuracy:= 0.8534967041015625 - Average Loss:= 0.006327613321940104\n",
            "\n",
            "Number of Epoch = 9 - Accuracy:= 0.8537467447916667 - Average Loss:= 0.00671036631266276\n",
            "\n",
            "Number of Epoch = 10 - Accuracy:= 0.8526691691080729 - Average Loss:= 0.007147650146484375\n",
            "\n",
            "Test Accuracy: 0.8324 - Loss: 0.0086\n",
            "\n",
            "==============1=============\n",
            "Number of Epoch = 1 - Accuracy:= 0.7075355021158855 - Average Loss:= 0.0061742360432942706\n",
            "\n",
            "Number of Epoch = 2 - Accuracy:= 0.8342205301920573 - Average Loss:= 0.004622159322102864\n",
            "\n",
            "Number of Epoch = 3 - Accuracy:= 0.8514285278320313 - Average Loss:= 0.004644813537597656\n",
            "\n",
            "Number of Epoch = 4 - Accuracy:= 0.8560417683919271 - Average Loss:= 0.004961702473958334\n",
            "\n",
            "Number of Epoch = 5 - Accuracy:= 0.8577018229166666 - Average Loss:= 0.005345411682128906\n",
            "\n",
            "Number of Epoch = 6 - Accuracy:= 0.8578203328450521 - Average Loss:= 0.005696559651692708\n",
            "\n",
            "Number of Epoch = 7 - Accuracy:= 0.858426513671875 - Average Loss:= 0.005912492879231771\n",
            "\n",
            "Number of Epoch = 8 - Accuracy:= 0.8564019775390626 - Average Loss:= 0.006369681803385417\n",
            "\n",
            "Number of Epoch = 9 - Accuracy:= 0.8572264607747396 - Average Loss:= 0.006708238220214844\n",
            "\n",
            "Number of Epoch = 10 - Accuracy:= 0.8562913004557292 - Average Loss:= 0.006928666687011718\n",
            "\n",
            "Test Accuracy: 0.8468 - Loss: 0.0076\n",
            "\n",
            "==============2=============\n",
            "Number of Epoch = 1 - Accuracy:= 0.7164120483398437 - Average Loss:= 0.006064776102701823\n",
            "\n",
            "Number of Epoch = 2 - Accuracy:= 0.8361555989583334 - Average Loss:= 0.004721058146158854\n",
            "\n",
            "Number of Epoch = 3 - Accuracy:= 0.8455562337239584 - Average Loss:= 0.004877606201171875\n",
            "\n",
            "Number of Epoch = 4 - Accuracy:= 0.8514952596028645 - Average Loss:= 0.005156914265950521\n",
            "\n",
            "Number of Epoch = 5 - Accuracy:= 0.8485772705078125 - Average Loss:= 0.0057630452473958335\n",
            "\n",
            "Number of Epoch = 6 - Accuracy:= 0.8482217407226562 - Average Loss:= 0.006333906046549479\n",
            "\n",
            "Number of Epoch = 7 - Accuracy:= 0.8518670654296875 - Average Loss:= 0.006561825561523437\n",
            "\n",
            "Number of Epoch = 8 - Accuracy:= 0.8489711507161458 - Average Loss:= 0.007193741353352864\n",
            "\n",
            "Number of Epoch = 9 - Accuracy:= 0.8524180094401042 - Average Loss:= 0.006970671081542969\n",
            "\n",
            "Number of Epoch = 10 - Accuracy:= 0.8553280639648437 - Average Loss:= 0.006888706461588541\n",
            "\n",
            "Test Accuracy: 0.8557 - Loss: 0.0072\n",
            "\n",
            "==============3=============\n",
            "Number of Epoch = 1 - Accuracy:= 0.7018915812174479 - Average Loss:= 0.00618702392578125\n",
            "\n",
            "Number of Epoch = 2 - Accuracy:= 0.8285419718424479 - Average Loss:= 0.004850824483235677\n",
            "\n",
            "Number of Epoch = 3 - Accuracy:= 0.8407543436686198 - Average Loss:= 0.004920311991373698\n",
            "\n",
            "Number of Epoch = 4 - Accuracy:= 0.844879862467448 - Average Loss:= 0.0052649500528971355\n",
            "\n",
            "Number of Epoch = 5 - Accuracy:= 0.8476580301920573 - Average Loss:= 0.00555179189046224\n",
            "\n",
            "Number of Epoch = 6 - Accuracy:= 0.8499735514322917 - Average Loss:= 0.00588363291422526\n",
            "\n",
            "Number of Epoch = 7 - Accuracy:= 0.8506418355305989 - Average Loss:= 0.006427929178873698\n",
            "\n",
            "Number of Epoch = 8 - Accuracy:= 0.8505939229329427 - Average Loss:= 0.006718002827962239\n",
            "\n",
            "Number of Epoch = 9 - Accuracy:= 0.8496126302083333 - Average Loss:= 0.007028215026855468\n",
            "\n",
            "Number of Epoch = 10 - Accuracy:= 0.8492776489257813 - Average Loss:= 0.00683710683186849\n",
            "\n",
            "Test Accuracy: 0.8246 - Loss: 0.0076\n",
            "\n",
            "==============4=============\n",
            "Number of Epoch = 1 - Accuracy:= 0.7156793212890625 - Average Loss:= 0.005964799499511719\n",
            "\n",
            "Number of Epoch = 2 - Accuracy:= 0.8341048177083333 - Average Loss:= 0.004647787475585937\n",
            "\n",
            "Number of Epoch = 3 - Accuracy:= 0.8480372111002604 - Average Loss:= 0.004815484110514323\n",
            "\n",
            "Number of Epoch = 4 - Accuracy:= 0.847835184733073 - Average Loss:= 0.0051722320556640625\n",
            "\n",
            "Number of Epoch = 5 - Accuracy:= 0.8466464233398437 - Average Loss:= 0.005734711710611979\n",
            "\n",
            "Number of Epoch = 6 - Accuracy:= 0.8456400044759115 - Average Loss:= 0.006317206319173177\n",
            "\n",
            "Number of Epoch = 7 - Accuracy:= 0.8503641764322917 - Average Loss:= 0.006598308308919271\n",
            "\n",
            "Number of Epoch = 8 - Accuracy:= 0.8506439208984375 - Average Loss:= 0.006922572835286458\n",
            "\n",
            "Number of Epoch = 9 - Accuracy:= 0.8461810302734375 - Average Loss:= 0.0077985646565755205\n",
            "\n",
            "Number of Epoch = 10 - Accuracy:= 0.8481678263346354 - Average Loss:= 0.008242380777994792\n",
            "\n",
            "Test Accuracy: 0.8242 - Loss: 0.0099\n",
            "\n",
            "==============5=============\n",
            "Number of Epoch = 1 - Accuracy:= 0.7024293009440105 - Average Loss:= 0.006279623921712239\n",
            "\n",
            "Number of Epoch = 2 - Accuracy:= 0.8312284342447916 - Average Loss:= 0.004769574483235677\n",
            "\n",
            "Number of Epoch = 3 - Accuracy:= 0.8405556233723959 - Average Loss:= 0.004888664245605469\n",
            "\n",
            "Number of Epoch = 4 - Accuracy:= 0.8444943237304687 - Average Loss:= 0.005189725748697917\n",
            "\n",
            "Number of Epoch = 5 - Accuracy:= 0.8495447794596355 - Average Loss:= 0.005434556070963542\n",
            "\n",
            "Number of Epoch = 6 - Accuracy:= 0.8504461161295573 - Average Loss:= 0.005777923075358073\n",
            "\n",
            "Number of Epoch = 7 - Accuracy:= 0.8522358194986979 - Average Loss:= 0.0059772420247395834\n",
            "\n",
            "Number of Epoch = 8 - Accuracy:= 0.8524222819010416 - Average Loss:= 0.006387406412760417\n",
            "\n",
            "Number of Epoch = 9 - Accuracy:= 0.8545299275716146 - Average Loss:= 0.006483386739095052\n",
            "\n",
            "Number of Epoch = 10 - Accuracy:= 0.8522903951009114 - Average Loss:= 0.00671109364827474\n",
            "\n",
            "Test Accuracy: 0.8486 - Loss: 0.0076\n",
            "\n",
            "==============6=============\n",
            "Number of Epoch = 1 - Accuracy:= 0.7064976501464844 - Average Loss:= 0.0061098876953125\n",
            "\n",
            "Number of Epoch = 2 - Accuracy:= 0.8341043090820313 - Average Loss:= 0.004684052530924479\n",
            "\n",
            "Number of Epoch = 3 - Accuracy:= 0.8452291361490886 - Average Loss:= 0.00486072998046875\n",
            "\n",
            "Number of Epoch = 4 - Accuracy:= 0.8486743672688802 - Average Loss:= 0.005166090901692708\n",
            "\n",
            "Number of Epoch = 5 - Accuracy:= 0.8447440083821615 - Average Loss:= 0.005731980387369792\n",
            "\n",
            "Number of Epoch = 6 - Accuracy:= 0.8489755249023437 - Average Loss:= 0.0059262537638346355\n",
            "\n",
            "Number of Epoch = 7 - Accuracy:= 0.8483522542317709 - Average Loss:= 0.0062361394246419274\n",
            "\n",
            "Number of Epoch = 8 - Accuracy:= 0.8523423258463542 - Average Loss:= 0.006424998474121093\n",
            "\n",
            "Number of Epoch = 9 - Accuracy:= 0.8522512817382812 - Average Loss:= 0.006993214925130209\n",
            "\n",
            "Number of Epoch = 10 - Accuracy:= 0.8551285807291666 - Average Loss:= 0.006939390563964844\n",
            "\n",
            "Test Accuracy: 0.8478 - Loss: 0.0072\n",
            "\n",
            "==============7=============\n",
            "Number of Epoch = 1 - Accuracy:= 0.7011495463053385 - Average Loss:= 0.0062996907552083335\n",
            "\n",
            "Number of Epoch = 2 - Accuracy:= 0.8329977416992187 - Average Loss:= 0.004862148030598958\n",
            "\n",
            "Number of Epoch = 3 - Accuracy:= 0.8419879150390625 - Average Loss:= 0.005101067097981771\n",
            "\n",
            "Number of Epoch = 4 - Accuracy:= 0.8469222005208333 - Average Loss:= 0.0054461898803710935\n",
            "\n",
            "Number of Epoch = 5 - Accuracy:= 0.8492327372233073 - Average Loss:= 0.005683745320638021\n",
            "\n",
            "Number of Epoch = 6 - Accuracy:= 0.8504098002115885 - Average Loss:= 0.006014115397135417\n",
            "\n",
            "Number of Epoch = 7 - Accuracy:= 0.8530552164713542 - Average Loss:= 0.00656682383219401\n",
            "\n",
            "Number of Epoch = 8 - Accuracy:= 0.8467147827148438 - Average Loss:= 0.006926826985677083\n",
            "\n",
            "Number of Epoch = 9 - Accuracy:= 0.8524282836914062 - Average Loss:= 0.007216411336263021\n",
            "\n",
            "Number of Epoch = 10 - Accuracy:= 0.8544871012369791 - Average Loss:= 0.006939629618326823\n",
            "\n",
            "Test Accuracy: 0.8400 - Loss: 0.0077\n",
            "\n",
            "==============8=============\n",
            "Number of Epoch = 1 - Accuracy:= 0.7099958292643229 - Average Loss:= 0.006199690755208333\n",
            "\n",
            "Number of Epoch = 2 - Accuracy:= 0.8290746561686198 - Average Loss:= 0.004816429646809896\n",
            "\n",
            "Number of Epoch = 3 - Accuracy:= 0.8443068440755208 - Average Loss:= 0.00498223876953125\n",
            "\n",
            "Number of Epoch = 4 - Accuracy:= 0.8437206013997396 - Average Loss:= 0.00540027821858724\n",
            "\n",
            "Number of Epoch = 5 - Accuracy:= 0.8513546244303385 - Average Loss:= 0.00561934814453125\n",
            "\n",
            "Number of Epoch = 6 - Accuracy:= 0.8515901692708333 - Average Loss:= 0.005819992065429687\n",
            "\n",
            "Number of Epoch = 7 - Accuracy:= 0.85462158203125 - Average Loss:= 0.0061043665568033855\n",
            "\n",
            "Number of Epoch = 8 - Accuracy:= 0.8542694091796875 - Average Loss:= 0.006429190063476562\n",
            "\n",
            "Number of Epoch = 9 - Accuracy:= 0.8515543619791667 - Average Loss:= 0.006688627115885417\n",
            "\n",
            "Number of Epoch = 10 - Accuracy:= 0.8504101053873698 - Average Loss:= 0.006984038798014323\n",
            "\n",
            "Test Accuracy: 0.8598 - Loss: 0.0070\n",
            "\n",
            "==============9=============\n",
            "Number of Epoch = 1 - Accuracy:= 0.7063086446126302 - Average Loss:= 0.00627055918375651\n",
            "\n",
            "Number of Epoch = 2 - Accuracy:= 0.8336868286132812 - Average Loss:= 0.004659999084472656\n",
            "\n",
            "Number of Epoch = 3 - Accuracy:= 0.8448734029134115 - Average Loss:= 0.0048277913411458335\n",
            "\n",
            "Number of Epoch = 4 - Accuracy:= 0.8494159952799479 - Average Loss:= 0.005174731953938802\n",
            "\n",
            "Number of Epoch = 5 - Accuracy:= 0.849263916015625 - Average Loss:= 0.005697226969401042\n",
            "\n",
            "Number of Epoch = 6 - Accuracy:= 0.8486510213216146 - Average Loss:= 0.006039867655436198\n",
            "\n",
            "Number of Epoch = 7 - Accuracy:= 0.8461460367838541 - Average Loss:= 0.006688771565755208\n",
            "\n",
            "Number of Epoch = 8 - Accuracy:= 0.8450565592447916 - Average Loss:= 0.0070171625773111975\n",
            "\n",
            "Number of Epoch = 9 - Accuracy:= 0.846998799641927 - Average Loss:= 0.007427926127115885\n",
            "\n",
            "Number of Epoch = 10 - Accuracy:= 0.8474129740397135 - Average Loss:= 0.007845508829752604\n",
            "\n",
            "Test Accuracy: 0.8391 - Loss: 0.0084\n",
            "\n",
            "\n",
            "Total time taken (in seconds): 1939.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/My Drive/Colab Notebooks/MLP/FMNIST_adam_l1.txt\",\"w\") as f:\n",
        "  for i in lines:\n",
        "    f.write(i)"
      ],
      "metadata": {
        "id": "8WN0tpYbfYdJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}