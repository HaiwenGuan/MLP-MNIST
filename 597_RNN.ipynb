{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaiwenGuan/MLP-MNIST/blob/main/597_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zydOvLSn9PQ1",
        "outputId": "b47b047f-b261-4c61-d40e-4092396dd279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "tf.random.set_seed(4208)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9wYGt3RGr2H4"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This script contains several functions used for data processing.\n",
        "'''\n",
        "\n",
        "#############################################################################\n",
        "# Import here useful libraries\n",
        "#############################################################################\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "import glob\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "\n",
        "def imdb2tfrecords(path_data='datasets/aclImdb/', min_word_frequency=5,\n",
        "                   max_words_review=700):\n",
        "    '''\n",
        "    This script processes the data and saves it in the default TensorFlow \n",
        "    file format: tfrecords.\n",
        "    \n",
        "    Args:\n",
        "        path_data: the path where the imdb data is stored.\n",
        "        min_word_frequency: the minimum frequency of a word, to keep it\n",
        "                            in the vocabulary.\n",
        "        max_words_review: the maximum number of words allowed in a review.\n",
        "    '''\n",
        "    # Get the filenames of the positive/negative reviews we will use\n",
        "    # for training the RNN\n",
        "    train_pos_files = glob.glob(path_data + 'train/pos/*')\n",
        "    train_neg_files = glob.glob(path_data + 'train/neg/*')\n",
        "\n",
        "    # Concatenate both positive and negative reviews filenames\n",
        "    train_files = train_pos_files + train_neg_files\n",
        "    \n",
        "    # List with all the reviews in the train dataset\n",
        "    reviews = [open(train_files[i],'r').read() for i in range(len(train_files))]\n",
        "    \n",
        "    # Remove HTML tags\n",
        "    reviews = [re.sub(r'<[^>]+>', ' ', review) for review in reviews]\n",
        "        \n",
        "    # Tokenize each review in part\n",
        "    reviews = [word_tokenize(review) for review in reviews]\n",
        "    \n",
        "    # Compute the length of each review\n",
        "    len_reviews = [len(review) for review in reviews]\n",
        "    pickle.dump(len_reviews, open(path_data + 'length_reviews.pkl', 'wb'))\n",
        "\n",
        "    # Flatten nested list\n",
        "    reviews = [word for review in reviews for word in review]\n",
        "    \n",
        "    # Compute the frequency of each word\n",
        "    word_frequency = pd.value_counts(reviews)\n",
        "    \n",
        "    # Keep only words with frequency higher than minimum\n",
        "    vocabulary = word_frequency[word_frequency>=min_word_frequency].index.tolist()\n",
        "    \n",
        "    # Add Unknown, Start and End token. \n",
        "    extra_tokens = ['Unknown_token', 'End_token']\n",
        "    vocabulary += extra_tokens\n",
        "    \n",
        "    # Create a word2idx dictionary\n",
        "    word2idx = {vocabulary[i]: i for i in range(len(vocabulary))}\n",
        "    \n",
        "    # Write word vocabulary to disk\n",
        "    pickle.dump(word2idx, open(path_data + 'word2idx.pkl', 'wb'))\n",
        "        \n",
        "    def text2tfrecords(filenames, writer, vocabulary, word2idx,\n",
        "                       max_words_review):\n",
        "        '''\n",
        "        Function to parse each review in part and write to disk\n",
        "        as a tfrecord.\n",
        "        \n",
        "        Args:\n",
        "            filenames: the paths of the review files.\n",
        "            writer: the writer object for tfrecords.\n",
        "            vocabulary: list with all the words included in the vocabulary.\n",
        "            word2idx: dictionary of words and their corresponding indexes.\n",
        "        '''\n",
        "        # Shuffle filenames\n",
        "        random.shuffle(filenames)\n",
        "        for filename in filenames:\n",
        "            review = open(filename, 'r').read()\n",
        "            review = re.sub(r'<[^>]+>', ' ', review)\n",
        "            review = word_tokenize(review)\n",
        "            # Reduce review to max words\n",
        "            review = review[-max_words_review:]\n",
        "            # Replace words with their equivalent index from word2idx\n",
        "            review = [word2idx[word] if word in vocabulary else \n",
        "                      word2idx['Unknown_token'] for word in review]\n",
        "            indexed_review = review + [word2idx['End_token']]\n",
        "            sequence_length = len(indexed_review)\n",
        "            target = 1 if filename.split('/')[-2]=='pos' else 0\n",
        "            # Create a Sequence Example to store our data in\n",
        "            ex = tf.train.SequenceExample()\n",
        "            # Add non-sequential features to our example\n",
        "            ex.context.feature['sequence_length'].int64_list.value.append(sequence_length)\n",
        "            ex.context.feature['target'].int64_list.value.append(target)\n",
        "            # Add sequential feature\n",
        "            token_indexes = ex.feature_lists.feature_list['token_indexes']\n",
        "            for token_index in indexed_review:\n",
        "                token_indexes.feature.add().int64_list.value.append(token_index)\n",
        "            writer.write(ex.SerializeToString())\n",
        "    \n",
        "    ##########################################################################     \n",
        "    # Write train data to tfrecords.This might take a while (~10 minutes)\n",
        "    ##########################################################################\n",
        "    train_writer = tf.python_io.TFRecordWriter(path_data + 'train.tfrecords')\n",
        "    text2tfrecords(train_files, train_writer, vocabulary, word2idx, \n",
        "                   max_words_review)\n",
        "\n",
        "    ##########################################################################\n",
        "    # Get the filenames of the reviews we will use for testing the RNN \n",
        "    ##########################################################################\n",
        "    test_pos_files = glob.glob(path_data + 'test/pos/*')\n",
        "    test_neg_files = glob.glob(path_data + 'test/neg/*')\n",
        "    test_files = test_pos_files + test_neg_files\n",
        "\n",
        "    ##########################################################################\n",
        "    # Write test data to tfrecords (~10 minutes)\n",
        "    ##########################################################################\n",
        "    test_writer = tf.python_io.TFRecordWriter('datasets/aclImdb/test.tfrecords')\n",
        "    text2tfrecords(test_files, test_writer, vocabulary, word2idx,\n",
        "                   max_words_review)\n",
        "\n",
        "\n",
        "def parse_imdb_sequence(record):\n",
        "    '''\n",
        "    Script to parse imdb tfrecords.\n",
        "    \n",
        "    Returns:\n",
        "        token_indexes: sequence of token indexes present in the review.\n",
        "        target: the target of the movie review.\n",
        "        sequence_length: the length of the sequence.\n",
        "    '''\n",
        "    context_features = {\n",
        "        'sequence_length': tf.io.FixedLenFeature([], dtype=tf.int64),\n",
        "        'target': tf.io.FixedLenFeature([], dtype=tf.int64),\n",
        "        }\n",
        "    sequence_features = {\n",
        "        'token_indexes': tf.io.FixedLenSequenceFeature([], dtype=tf.int64),\n",
        "        }\n",
        "    context_parsed, sequence_parsed = tf.io.parse_single_sequence_example(record, \n",
        "        context_features=context_features, sequence_features=sequence_features)\n",
        "        \n",
        "    return (sequence_parsed['token_indexes'], context_parsed['target'],\n",
        "            context_parsed['sequence_length'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4CrTBL49Xr3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ck1549pm9b1P",
        "outputId": "1568b035-e870-4be3-d5ae-e54bc6bbcab5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXNUlEQVR4nO3df7BfdZ3f8efL8GMdpUtY7tIQUhNtXDfaMTJfkVarrtYQmLbBWetirWRdZrK7A61Otbuwdgrq2q7bKlM6SouFNToqZUSHrMMWI2vxR8uPGzcgAVmugJtkA7kaQFgtu+C7f3w/t3693Jt7k9zcH9/zfMycued8zuf8+OTcvL7nfs75npOqQpLUDc9Z6B2QJM0fQ1+SOsTQl6QOMfQlqUMMfUnqEENfkjrE0JeaJKuTVJJjFmDbb0/y5fnerrrH0NeikOShJP9o2Lc5nar6TFVtWOj90PAz9KU5kmTZQu+DNBNDX4takuckuTjJd5P8IMl1SU5q8ya6YzYn+Ysk30/yvoFln5tka5JHk9yb5HeS7GnzPg38HeCPkzyZ5HcGNvv2qdY3xb59MsmVSW5M8lfAryQ5Ncn1ScaTPJjkX7W6pyb58cS+t7JXtG0cm+TXk3xjYN5LkmxPciDJfUne2srXJHksyXPa9CeS7B9Y7tNJ3t3Gfz3JA0meaPvy9iM5FhoOhr4Wu38JnAu8DjgVeBT42KQ6rwF+CXgj8O+S/HIrvxRYDbwQeBPwLyYWqKp3AH8B/JOqen5V/eEs1jeVfw58CDgB+N/AHwN3Aivb8u9OclZV/SXwf4BfnbTs56vqbwZXmOR5wHbgs8AvAucBH0+yrqoeBH4IvKJVfy3w5MA+vg64pa3jCuDsqjoB+AfAzoO0Qx1h6Gux+y3gfVW1p6qeAi4D3jLpYuv7q+rHVXUn/cB9eSt/K/Dvq+rRqtpDPwRnY7r1TeWGqvpmVf0E+HvASFV9oKr+uqoeAD5BP7ShH+JvA0iSVv7ZKdb5j4GHquqPqurpqvoz4Hrgn7X5twCvS/K32/Tn2/Qa4G+1fQb4CfCyJM+tqn1VtWuW7dcQm/e7FKRD9ALgi0l+MlD2DHDKwPTDA+M/Ap7fxk8Fdg/MGxw/mOnWN5XBdb4AODXJYwNly4Cvt/Hrgf+SZAXwYvqh/HWe7QXAqyat5xjg0238FuCfAnuArwH/C3gH8H+Br7cPoL9K8mvAe4Grk3wTeE9VfecgbVEHGPpa7HYDv1FV35w8I8nqGZbdB5wG3NOmV02aPxePmB1cx27gwapaO2XFqkfbbZm/BvwycG1N/Zjb3cAtVfWmabZ5C/Af6Yf+LcA3gP9KP/RvGdjeTcBNSZ4L/D79vzr+4SG0TUPI7h0tJscm+bmB4Rj6YfahJC8ASDKSZNMs13cdcEmS5UlWAhdNmv8I/f7+uXI78ESS320XkZcleVmSVw7U+SxwPvAWpu7aAfgS8OIk72gXeY9N8sqJfvuquh/4Mf1rFLdU1Q9bW36VFvpJTkmyqfXtPwU8Sf8vC3Wcoa/F5Eb6YTYxXAb8Z2Ab8OUkTwC3Aq+a5fo+QP9s+EHgK/T7vp8amP8fgH/b7oZ575HufFU9Q78/fn3b5veB/w78/EC1bcBa4OF2zWCq9TwBbKDf5/+X9LubPgwcP1DtFuAHVbV7YDrAt9r0c4B/3ZY/QP8C728fWQs1DOJLVNQVSX4bOK+qXrfQ+yItFM/0NbSSrEjy6nav/y8B7wG+uND7JS0kL+RqmB0H/DdgDfAYcC3w8QXdI2mB2b0jSR1i944kdcii7t45+eSTa/Xq1Qu9G5K0pOzYseP7VTUy1bxFHfqrV69mdHR0oXdDkpaUJN+bbp7dO5LUIYa+JHWIoS9JHWLoS1KHGPqS1CGGviR1yIyh3x5xe3uSO5PsSvL+Vv7J9t7NnW1Y38qT5IokY0nuSnL6wLo2J7m/DZuPXrMkSVOZzX36TwFvqKonkxwLfCPJn7R5/6aqPj+p/tn0Hx27lv4jcK+k/xagk+i/s7RH/8UTO5Jsq6pH56IhkqSZzXimX31Ptslj23CwB/ZsAj7VlrsVOLG9Hu4sYHtVHWhBvx3YeGS7L0k6FLPq029vANoJ7Kcf3Le1WR9qXTiXJ5l4wcNKfva9oXta2XTlk7e1JcloktHx8fFDbM7cSX46SNKwmFXoV9UzVbWe/vtGz0jyMuAS4CXAK4GTgN+dix2qqquqqldVvZGRKR8dIUk6TId0905VPQZ8FdhYVftaF85TwB8BZ7Rqe/nZF1Cf1sqmK5ckzZPZ3L0zkuTENv5c4E3Ad1o/PUkCnAvc3RbZBpzf7uI5E3i8qvYBNwEb2kuql9N/B+hNc94iSdK0ZnP3zgpga5Jl9D8krquqLyX50yQj9F/GvBP4rVb/RuAcYAz4EfBOgKo6kOSDwB2t3geq6sDcNUWSNJNF/easXq9XC/Vo5cELuIv4n0iSniXJjqrqTTXPb+RKUocY+pLUIYa+JHWIoS9JHWLoS1KHGPqS1CGGviR1iKEvSR0ym2/kdoZP1JQ07DzTl6QOMfQlqUMMfUnqEENfkjrE0JekDjH0JalDDH1J6hBDX5I6xNCXpA4x9CWpQwx9SeqQGUM/yc8luT3JnUl2JXl/K1+T5LYkY0n+R5LjWvnxbXqszV89sK5LWvl9Sc46Wo2SJE1tNmf6TwFvqKqXA+uBjUnOBD4MXF5Vfxd4FLig1b8AeLSVX97qkWQdcB7wUmAj8PEky+ayMZKkg5sx9KvvyTZ5bBsKeAPw+Va+FTi3jW9q07T5b0ySVn5tVT1VVQ8CY8AZc9IKSdKszKpPP8myJDuB/cB24LvAY1X1dKuyB1jZxlcCuwHa/MeBXxgsn2KZwW1tSTKaZHR8fPzQWyRJmtasQr+qnqmq9cBp9M/OX3K0dqiqrqqqXlX1RkZGjtZmJKmTDununap6DPgq8PeBE5NMvITlNGBvG98LrAJo838e+MFg+RTLSJLmwWzu3hlJcmIbfy7wJuBe+uH/llZtM3BDG9/Wpmnz/7SqqpWf1+7uWQOsBW6fq4ZIkmY2m9clrgC2tjttngNcV1VfSnIPcG2S3wf+DLi61b8a+HSSMeAA/Tt2qKpdSa4D7gGeBi6sqmfmtjmSpINJ/yR8cer1ejU6Ojpv25vuHbmL+J9Ikp4lyY6q6k01z2/kSlKHGPqS1CGGviR1iKEvSR1i6EtShxj6ktQhhr4kdYihL0kdYuhLUocY+pLUIYa+JHWIoS9JHWLoS1KHGPqS1CGGviR1iKEvSR1i6EtShxj6ktQhhr4kdYihL0kdYuhLUofMGPpJViX5apJ7kuxK8q5WflmSvUl2tuGcgWUuSTKW5L4kZw2Ub2xlY0kuPjpNkiRN55hZ1HkaeE9VfSvJCcCOJNvbvMur6j8NVk6yDjgPeClwKvCVJC9usz8GvAnYA9yRZFtV3TMXDTmakp+OVy3cfkjSkZox9KtqH7CvjT+R5F5g5UEW2QRcW1VPAQ8mGQPOaPPGquoBgCTXtrqLPvQlaVgcUp9+ktXAK4DbWtFFSe5Kck2S5a1sJbB7YLE9rWy68snb2JJkNMno+Pj4oeyeJGkGsw79JM8HrgfeXVU/BK4EXgSsp/+XwEfmYoeq6qqq6lVVb2RkZC5WKUlqZtOnT5Jj6Qf+Z6rqCwBV9cjA/E8AX2qTe4FVA4uf1so4SLkkaR7M5u6dAFcD91bVRwfKVwxUezNwdxvfBpyX5Pgka4C1wO3AHcDaJGuSHEf/Yu+2uWmGJGk2ZnOm/2rgHcC3k+xsZb8HvC3JeqCAh4DfBKiqXUmuo3+B9mngwqp6BiDJRcBNwDLgmqraNYdtkSTNILWI70Hs9Xo1Ojo6b9sbvDVzOov4n0uSAEiyo6p6U83zG7mS1CGGviR1iKEvSR1i6EtShxj6ktQhhr4kdYihL0kdYuhLUocY+pLUIYa+JHWIoS9JHWLoS1KHGPqS1CGGviR1iKEvSR1i6EtShxj6ktQhhr4kdYihL0kdYuhLUofMGPpJViX5apJ7kuxK8q5WflKS7Unubz+Xt/IkuSLJWJK7kpw+sK7Nrf79STYfvWZJkqYymzP9p4H3VNU64EzgwiTrgIuBm6tqLXBzmwY4G1jbhi3AldD/kAAuBV4FnAFcOvFBIUmaHzOGflXtq6pvtfEngHuBlcAmYGurthU4t41vAj5VfbcCJyZZAZwFbK+qA1X1KLAd2DinrZEkHdQh9eknWQ28ArgNOKWq9rVZDwOntPGVwO6Bxfa0sunKJ29jS5LRJKPj4+OHsnuSpBnMOvSTPB+4Hnh3Vf1wcF5VFVBzsUNVdVVV9aqqNzIyMhernFPJTwdJWmpmFfpJjqUf+J+pqi+04kdatw3t5/5WvhdYNbD4aa1sunJJ0jyZzd07Aa4G7q2qjw7M2gZM3IGzGbhhoPz8dhfPmcDjrRvoJmBDkuXtAu6GViZJmifHzKLOq4F3AN9OsrOV/R7wB8B1SS4Avge8tc27ETgHGAN+BLwToKoOJPkgcEer94GqOjAnrZAkzUr63fGLU6/Xq9HR0Xnb3qH20y/ifzpJHZZkR1X1pprnN3IlqUMMfUnqEENfkjrE0JekDjH0JalDDH1J6hBDX5I6xNCXpA4x9CWpQwx9SeoQQ1+SOsTQl6QOMfQlqUMMfUnqEENfkjrE0JekDjH0JalDDH1J6hBDX5I6xNCXpA4x9CWpQ2YM/STXJNmf5O6BssuS7E2ysw3nDMy7JMlYkvuSnDVQvrGVjSW5eO6bIkmayWzO9D8JbJyi/PKqWt+GGwGSrAPOA17alvl4kmVJlgEfA84G1gFva3UlSfPomJkqVNXXkqye5fo2AddW1VPAg0nGgDPavLGqegAgybWt7j2HvMeSpMN2JH36FyW5q3X/LG9lK4HdA3X2tLLpyp8lyZYko0lGx8fHj2D3JEmTHW7oXwm8CFgP7AM+Mlc7VFVXVVWvqnojIyNztVpJErPo3plKVT0yMZ7kE8CX2uReYNVA1dNaGQcplyTNk8M600+yYmDyzcDEnT3bgPOSHJ9kDbAWuB24A1ibZE2S4+hf7N12+Lu9OCQ/HSRpKZjxTD/J54DXAycn2QNcCrw+yXqggIeA3wSoql1JrqN/gfZp4MKqeqat5yLgJmAZcE1V7Zrz1kiSDipVtdD7MK1er1ejo6Pztr0jOWNfxP+MkjomyY6q6k01z2/kSlKHGPqS1CGGviR1yGHdsjlMvPNGUpd4pi9JHWLoS1KHGPqS1CGGviR1iKEvSR1i6EtShxj6ktQhhr4kdUjnv5w1Vwa/5OXD1yQtVp7pS1KHGPqS1CGGviR1iKEvSR3SyQu5PllTUld5pi9JHWLoS1KHzBj6Sa5Jsj/J3QNlJyXZnuT+9nN5K0+SK5KMJbkryekDy2xu9e9PsvnoNEeSdDCzOdP/JLBxUtnFwM1VtRa4uU0DnA2sbcMW4Erof0gAlwKvAs4ALp34oJAkzZ8ZQ7+qvgYcmFS8CdjaxrcC5w6Uf6r6bgVOTLICOAvYXlUHqupRYDvP/iCRJB1lh3v3zilVta+NPwyc0sZXArsH6u1pZdOVDyUfySBpsTriC7lVVcCcRVuSLUlGk4yOj4/P1WolSRx+6D/Sum1oP/e38r3AqoF6p7Wy6cqfpaquqqpeVfVGRkYOc/ckSVM53NDfBkzcgbMZuGGg/Px2F8+ZwOOtG+gmYEOS5e0C7oZWJkmaRzP26Sf5HPB64OQke+jfhfMHwHVJLgC+B7y1Vb8ROAcYA34EvBOgqg4k+SBwR6v3gaqafHFYknSUpRbxlcZer1ejo6Nzvt75fAzDIv7nlTSkkuyoqt5U8/xGriR1iKEvSR1i6EtShxj6ktQhnXye/nzy27mSFpPOhL4vTpEku3ckqVMMfUnqEENfkjrE0JekDjH0JalDOnP3zmLg7ZuSFppn+pLUIYa+JHWIoS9JHWLoS1KHGPqS1CHevbNAvJNH0kLwTF+SOsTQl6QOsXtnEbCrR9J8OaIz/SQPJfl2kp1JRlvZSUm2J7m//VzeypPkiiRjSe5KcvpcNECSNHtz0b3zK1W1vqp6bfpi4OaqWgvc3KYBzgbWtmELcOUcbFuSdAiORp/+JmBrG98KnDtQ/qnquxU4McmKo7B9SdI0jjT0C/hykh1JtrSyU6pqXxt/GDilja8Edg8su6eV/YwkW5KMJhkdHx8/wt2TJA060gu5r6mqvUl+Edie5DuDM6uqkhzSpcmqugq4CqDX63lZU5Lm0BGd6VfV3vZzP/BF4AzgkYlum/Zzf6u+F1g1sPhprUySNE8OO/STPC/JCRPjwAbgbmAbsLlV2wzc0Ma3Aee3u3jOBB4f6AaSJM2DI+neOQX4Yvo3mR8DfLaq/meSO4DrklwAfA94a6t/I3AOMAb8CHjnEWx7aHnPvqSj6bBDv6oeAF4+RfkPgDdOUV7AhYe7PUnSkfMbuYuYZ/2S5prP3pGkDvFMf4nwrF/SXPBMX5I6xNCXpA4x9CWpQ+zTX4Ls35d0uAz9JW7wAwD8EJB0cHbvSFKHeKY/ZOz6kXQwnulLUocY+pLUIXbvDDG7eiRNZuh3hB8AksDQ7yQ/AKTuMvQ7zg8AqVsMff1/fgBIw8/Q15T8AJCGk6GvGU1+1MMEPwykpWeoQ3+6sNLc8MNAWnqGOvS1MGbzYesHg7Qw5v0buUk2JrkvyViSi+d7+1ockqmH2dSZbX1JzzavoZ9kGfAx4GxgHfC2JOvmcx+0uB1qaB+svh8A0rPN95n+GcBYVT1QVX8NXAtsmud9UAf5ASD1zXef/kpg98D0HuBVgxWSbAG2tMknk9x3GNs5Gfj+Ye3h0mD7jsAiCH6P39K1VNr2gulmLLoLuVV1FXDVkawjyWhV9eZolxYd27e02b6laxjaNt/dO3uBVQPTp7UySdI8mO/QvwNYm2RNkuOA84Bt87wPktRZ89q9U1VPJ7kIuAlYBlxTVbuOwqaOqHtoCbB9S5vtW7qWfNtSfktGkjrD1yVKUocY+pLUIUMX+sPymIckDyX5dpKdSUZb2UlJtie5v/1c3sqT5IrW5ruSnL6we/9sSa5Jsj/J3QNlh9yeJJtb/fuTbF6Itkw2TdsuS7K3Hb+dSc4ZmHdJa9t9Sc4aKF+Uv7tJViX5apJ7kuxK8q5WPizHb7r2Dc0x/BlVNTQD/YvD3wVeCBwH3AmsW+j9Osy2PAScPKnsD4GL2/jFwIfb+DnAnwABzgRuW+j9n6I9rwVOB+4+3PYAJwEPtJ/L2/jyRdq2y4D3TlF3Xfu9PB5Y035fly3m311gBXB6Gz8B+PPWjmE5ftO1b2iO4eAwbGf6w/6Yh03A1ja+FTh3oPxT1XcrcGKSFQuxg9Opqq8BByYVH2p7zgK2V9WBqnoU2A5sPPp7f3DTtG06m4Brq+qpqnoQGKP/e7tof3eral9VfauNPwHcS//b9cNy/KZr33SW3DEcNGyhP9VjHg528BazAr6cZEd7NAXAKVW1r40/DJzSxpdquw+1PUutnRe17o1rJro+WOJtS7IaeAVwG0N4/Ca1D4bwGA5b6A+T11TV6fSfSHphktcOzqz+35lDc7/tsLUHuBJ4EbAe2Ad8ZGF358gleT5wPfDuqvrh4LxhOH5TtG/ojiEMX+gPzWMeqmpv+7kf+CL9Px0fmei2aT/3t+pLtd2H2p4l086qeqSqnqmqnwCfoH/8YIm2Lcmx9APxM1X1hVY8NMdvqvYN2zGcMGyhPxSPeUjyvCQnTIwDG4C76bdl4o6HzcANbXwbcH67a+JM4PGBP7sXs0Ntz03AhiTL25/aG1rZojPpmsqb6R8/6LftvCTHJ1kDrAVuZxH/7iYJcDVwb1V9dGDWUBy/6do3TMfwZyz0leS5HujfOfDn9K+iv2+h9+cw2/BC+lf+7wR2TbQD+AXgZuB+4CvASa089F9O813g20BvodswRZs+R/9P5L+h39d5weG0B/gN+hfOxoB3LnS7DtK2T7d9v4v+f/wVA/Xf19p2H3D2Yv/dBV5Dv+vmLmBnG84ZouM3XfuG5hgODj6GQZI6ZNi6dyRJB2HoS1KHGPqS1CGGviR1iKEvSR1i6EtShxj6ktQh/w+2Xi6lHlboewAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "length_reviews = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/RNN_data/length_reviews.pkl', 'rb'))\n",
        "pd.DataFrame(length_reviews, columns=['Length reviews']).hist(bins=100, color='blue');\n",
        "plt.grid(False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Faxi4EVf9ezf"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.TFRecordDataset('/content/drive/MyDrive/Colab Notebooks/RNN_data/train.tfrecords')\n",
        "train_dataset = train_dataset.map(parse_imdb_sequence).shuffle(buffer_size=10000)\n",
        "val_dataset = train_dataset.take(5000)\n",
        "train_dataset = train_dataset.skip(5000)\n",
        "val_dataset = val_dataset.padded_batch(512, padded_shapes=([None],[],[]))\n",
        "train_dataset = train_dataset.padded_batch(512, padded_shapes=([None],[],[]))\n",
        "test_dataset = tf.data.TFRecordDataset('/content/drive/MyDrive/Colab Notebooks/RNN_data/test.tfrecords')\n",
        "test_dataset = test_dataset.map(parse_imdb_sequence).shuffle(buffer_size=10000)\n",
        "test_dataset = test_dataset.padded_batch(512, padded_shapes=([None],[],[]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "H_6KF3Ui9hlb"
      },
      "outputs": [],
      "source": [
        "# Read the word vocabulary\n",
        "word2idx = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/RNN_data/word2idx.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RmzecBc59kYE"
      },
      "outputs": [],
      "source": [
        "class RNNModel(tf.keras.Model):\n",
        "    def __init__(self, embedding_size=100, cell_size=64, dense_size=128, \n",
        "                 num_classes=2, vocabulary_size=None, rnn_cell='lstm',\n",
        "                 device='cpu:0', checkpoint_directory=None):\n",
        "        ''' Define the parameterized layers used during forward-pass, the device\n",
        "            where you would like to run the computation on and the checkpoint\n",
        "            directory. Additionaly, you can also modify the default size of the \n",
        "            network.\n",
        "            \n",
        "            Args:\n",
        "                embedding_size: the size of the word embedding.\n",
        "                cell_size: RNN cell size.\n",
        "                dense_size: the size of the dense layer.\n",
        "                num_classes: the number of labels in the network.\n",
        "                vocabulary_size: the size of the word vocabulary.\n",
        "                rnn_cell: string, either 'lstm' or 'ugrnn'.\n",
        "                device: string, 'cpu:n' or 'gpu:n' (n can vary). Default, 'cpu:0'.\n",
        "                checkpoint_directory: the directory where you would like to save or \n",
        "                                      restore a model.\n",
        "        '''\n",
        "        super(RNNModel, self).__init__()\n",
        "        \n",
        "        # Weights initializer function\n",
        "        w_initializer = tf.compat.v1.keras.initializers.glorot_uniform()\n",
        "    \n",
        "        # Biases initializer function\n",
        "        b_initializer = tf.zeros_initializer()\n",
        "        \n",
        "        # Initialize weights for word embeddings \n",
        "        self.embeddings = tf.keras.layers.Embedding(vocabulary_size, embedding_size, \n",
        "                                                    embeddings_initializer=w_initializer)\n",
        "        \n",
        "        # Dense layer initialization\n",
        "        self.dense_layer = tf.keras.layers.Dense(dense_size, activation=tf.nn.relu, \n",
        "                                                 kernel_initializer=w_initializer, \n",
        "                                                 bias_initializer=b_initializer)\n",
        "        \n",
        "        # Predictions layer initialization\n",
        "        self.pred_layer = tf.keras.layers.Dense(num_classes, activation=None, \n",
        "                                                kernel_initializer=w_initializer, \n",
        "                                                bias_initializer=b_initializer)\n",
        "        \n",
        "        # Basic LSTM cell\n",
        "        if rnn_cell=='lstm':\n",
        "            self.rnn_cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(cell_size)\n",
        "        # Else RNN cell\n",
        "        else:\n",
        "            self.rnn_cell = tf.compat.v1.nn.rnn_cell.BasicRNNCell(cell_size)\n",
        "            \n",
        "        # Define the device\n",
        "        self.device = device\n",
        "        \n",
        "        # Define the checkpoint directory\n",
        "        self.checkpoint_directory = checkpoint_directory\n",
        "        \n",
        "    def predict(self, X, seq_length, is_training):\n",
        "        '''\n",
        "        Predicts the probability of each class, based on the input sample.\n",
        "\n",
        "        Args:\n",
        "            X: 2D tensor of shape (batch_size, time_steps).\n",
        "            seq_length: the length of each sequence in the batch.\n",
        "            is_training: Boolean. Either the network is predicting in\n",
        "                         training mode or not.\n",
        "        '''\n",
        "        \n",
        "        # Get the number of samples within a batch\n",
        "        num_samples = tf.shape(X)[0]\n",
        "\n",
        "        # Initialize LSTM cell state with zeros\n",
        "        state = self.rnn_cell.zero_state(num_samples, dtype=tf.float32)\n",
        "        \n",
        "        # Get the embedding of each word in the sequence\n",
        "        embedded_words = self.embeddings(X)\n",
        "        \n",
        "        # Unstack the embeddings\n",
        "        unstacked_embeddings = tf.unstack(embedded_words, axis=1)\n",
        "        \n",
        "        # Iterate through each timestep and append the predictions\n",
        "        outputs = []\n",
        "        for input_step in unstacked_embeddings:\n",
        "            output, state = self.rnn_cell(input_step, state)\n",
        "            outputs.append(output)\n",
        "            \n",
        "        # Stack outputs to (batch_size, time_steps, cell_size)\n",
        "        outputs = tf.stack(outputs, axis=1)\n",
        "        \n",
        "        # Extract the output of the last time step, of each sample\n",
        "        idxs_last_output = tf.stack([tf.range(num_samples), \n",
        "                                     tf.cast(seq_length-1, tf.int32)], axis=1)\n",
        "        final_output = tf.gather_nd(outputs, idxs_last_output)\n",
        "        \n",
        "        # Add dropout for regularization\n",
        "        #dropped_output = tf.compat.v1.layers.Dropout(final_output, rate=0.3, training=is_training)\n",
        "        \n",
        "        # Pass the last cell state through a dense layer (ReLU activation)\n",
        "        dense = self.dense_layer(final_output)\n",
        "        \n",
        "        # Compute the unnormalized log probabilities\n",
        "        logits = self.pred_layer(dense)\n",
        "        return logits\n",
        "    def loss_fn(self, X, y, seq_length, is_training):\n",
        "        \"\"\" Defines the loss function used during \n",
        "            training.         \n",
        "        \"\"\"\n",
        "        preds = self.predict(X, seq_length, is_training)\n",
        "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=preds)\n",
        "        return loss\n",
        "    \n",
        "    def grads_fn(self, X, y, seq_length, is_training):\n",
        "        \"\"\" Dynamically computes the gradients of the loss value\n",
        "            with respect to the parameters of the model, in each\n",
        "            forward pass.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.loss_fn(X, y, seq_length, is_training)\n",
        "        return tape.gradient(loss, self.variables)\n",
        "    \n",
        "    def restore_model(self):\n",
        "        \"\"\" Function to restore trained model.\n",
        "        \"\"\"\n",
        "        with tf.device(self.device):\n",
        "            # Run the model once to initialize variables\n",
        "            dummy_input = tf.constant(tf.zeros((1,1)))\n",
        "            dummy_length = tf.constant(1, shape=(1,))\n",
        "            dummy_pred = self.predict(dummy_input, dummy_length, False)\n",
        "            # Restore the variables of the model\n",
        "            saver = tf.compat.v1.train.Saver(self.variables)\n",
        "            saver.restore(tf.train.latest_checkpoint\n",
        "                          (self.checkpoint_directory))\n",
        "    \n",
        "    def save_model(self, global_step=0):\n",
        "        \"\"\" Function to save trained model.\n",
        "        \"\"\"\n",
        "        tf.compat.v1.train.Saver(self.variables).save(save_path=self.checkpoint_directory, \n",
        "                                       global_step=global_step)   \n",
        "        \n",
        "    def fit(self, training_data, eval_data, optimizer, num_epochs=500, \n",
        "            early_stopping_rounds=10, verbose=10, train_from_scratch=False):\n",
        "        \"\"\" Function to train the model, using the selected optimizer and\n",
        "            for the desired number of epochs. You can either train from scratch\n",
        "            or load the latest model trained. Early stopping is used in order to\n",
        "            mitigate the risk of overfitting the network.\n",
        "            \n",
        "            Args:\n",
        "                training_data: the data you would like to train the model on.\n",
        "                                Must be in the tf.data.Dataset format.\n",
        "                eval_data: the data you would like to evaluate the model on.\n",
        "                            Must be in the tf.data.Dataset format.\n",
        "                optimizer: the optimizer used during training.\n",
        "                num_epochs: the maximum number of iterations you would like to \n",
        "                            train the model.\n",
        "                early_stopping_rounds: stop training if the accuracy on the eval \n",
        "                                       dataset does not increase after n epochs.\n",
        "                verbose: int. Specify how often to print the loss value of the network.\n",
        "                train_from_scratch: boolean. Whether to initialize variables of the\n",
        "                                    the last trained model or initialize them\n",
        "                                    randomly.\n",
        "        \"\"\" \n",
        "    \n",
        "        if train_from_scratch==False:\n",
        "            self.restore_model()\n",
        "        \n",
        "        # Initialize best_acc. This variable will store the highest accuracy\n",
        "        # on the eval dataset.\n",
        "        best_acc = 0\n",
        "        \n",
        "        # Initialize classes to update the mean accuracy of train and eval\n",
        "        train_acc = tf.keras.metrics.Accuracy('train_acc')\n",
        "        eval_acc = tf.keras.metrics.Accuracy('eval_acc')\n",
        "        \n",
        "        # Initialize dictionary to store the accuracy history\n",
        "        self.history = {}\n",
        "        self.history['train_acc'] = []\n",
        "        self.history['eval_acc'] = []\n",
        "        \n",
        "        # Begin training\n",
        "        with tf.device(self.device):\n",
        "            for i in range(num_epochs):\n",
        "                # Training with gradient descent\n",
        "                for step, (X, y, seq_length) in enumerate(training_data):\n",
        "                    grads = self.grads_fn(X, y, seq_length, True)\n",
        "                    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "                    \n",
        "                # Check accuracy train dataset\n",
        "                for step, (X, y, seq_length) in enumerate(training_data):\n",
        "                    logits = self.predict(X, seq_length, False)\n",
        "                    preds = tf.argmax(logits, axis=1)\n",
        "                    train_acc(preds, y)\n",
        "                self.history['train_acc'].append(train_acc.result().numpy())\n",
        "                # Reset metrics\n",
        "                train_acc.reset_states()\n",
        "\n",
        "                # Check accuracy eval dataset\n",
        "                for step, (X, y, seq_length) in enumerate(eval_data):\n",
        "                    logits = self.predict(X, seq_length, False)\n",
        "                    logits = self.predict(X, seq_length, False)\n",
        "                    preds = tf.argmax(logits, axis=1)\n",
        "                    eval_acc(preds, y)\n",
        "                self.history['eval_acc'].append(eval_acc.result().numpy())\n",
        "                # Reset metrics\n",
        "                eval_acc.reset_states()\n",
        "                \n",
        "                # Print train and eval accuracy\n",
        "                if (i==0) | ((i+1)%verbose==0):\n",
        "                    print('Train accuracy at epoch %d: ' %(i+1), self.history['train_acc'][-1])\n",
        "                    print('Eval accuracy at epoch %d: ' %(i+1), self.history['eval_acc'][-1])\n",
        "\n",
        "                # Check for early stopping\n",
        "                if self.history['eval_acc'][-1]>best_acc:\n",
        "                    best_acc = self.history['eval_acc'][-1]\n",
        "                    count = early_stopping_rounds\n",
        "                else:\n",
        "                    count -= 1\n",
        "                if count==0:\n",
        "                    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prro5qbP-Wso"
      },
      "source": [
        "With LSTM units"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NBopXsE-KT1",
        "outputId": "67fefbcc-148f-42d7-ff38-deac7da8297b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n"
          ]
        }
      ],
      "source": [
        "# Specify the path where you want to save/restore the trained variables.\n",
        "checkpoint_directory = 'models_checkpoints/ImdbRNN/'\n",
        "\n",
        "# Use the GPU if available.\n",
        "device = 'gpu:0'\n",
        "\n",
        "# Define optimizer.\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-4)\n",
        "\n",
        "# Instantiate model. This doesn't initialize the variables yet.\n",
        "lstm_model = RNNModel(vocabulary_size=len(word2idx), device=device, \n",
        "                      checkpoint_directory=checkpoint_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ-k-WKG-Mlx",
        "outputId": "4a8cda5d-db01-4ef9-ff6f-49a56d44d0f9"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:756: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  shape=[input_depth + h_depth, 4 * self._num_units])\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:760: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=tf.compat.v1.zeros_initializer(dtype=self.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy at epoch 1:  0.7185\n",
            "Eval accuracy at epoch 1:  0.7176\n",
            "Train accuracy at epoch 2:  0.7983\n",
            "Eval accuracy at epoch 2:  0.7924\n",
            "Train accuracy at epoch 3:  0.85275\n",
            "Eval accuracy at epoch 3:  0.846\n",
            "Train accuracy at epoch 4:  0.91315\n",
            "Eval accuracy at epoch 4:  0.9072\n",
            "Train accuracy at epoch 5:  0.93565\n",
            "Eval accuracy at epoch 5:  0.9294\n",
            "Train accuracy at epoch 6:  0.9523\n",
            "Eval accuracy at epoch 6:  0.9402\n",
            "Train accuracy at epoch 7:  0.9647\n",
            "Eval accuracy at epoch 7:  0.9552\n",
            "Train accuracy at epoch 8:  0.97265\n",
            "Eval accuracy at epoch 8:  0.9624\n",
            "Train accuracy at epoch 9:  0.9785\n",
            "Eval accuracy at epoch 9:  0.9708\n",
            "Train accuracy at epoch 10:  0.9848\n",
            "Eval accuracy at epoch 10:  0.9774\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "lstm_model.fit(train_dataset, val_dataset, optimizer, num_epochs=10, \n",
        "                early_stopping_rounds=5, verbose=1, train_from_scratch=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = tf.keras.metrics.Accuracy('test_acc')\n",
        "test_acc_lst = []\n",
        "for step, (X, y, seq_length) in enumerate(test_dataset):\n",
        "  logits = lstm_model.predict(X, seq_length, False)\n",
        "  preds = tf.argmax(logits, axis=1)\n",
        "  test_acc(preds,y)\n",
        "  test_acc_lst.append(test_acc.result().numpy())\n",
        "  test_acc.reset_states()"
      ],
      "metadata": {
        "id": "407K_E4oZ04r"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test accuracy: ', sum(test_acc_lst)/(len(test_acc_lst)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd_xkpZn3KFG",
        "outputId": "770ce192-ebca-4dcf-9cd3-0c4d806158bf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy:  0.8769132653061225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "PIVEB6sT-P_f"
      },
      "outputs": [],
      "source": [
        "#lstm_model.save_model()\n",
        "checkpoint = tf.train.Checkpoint(lstm_model)\n",
        "save_path = checkpoint.save(checkpoint_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbSqtWWP-ZQO"
      },
      "source": [
        "Training with RNN cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMzMxV-K-fcy"
      },
      "outputs": [],
      "source": [
        "# Define optimizer.\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-4)\n",
        "\n",
        "# Instantiate model. This doesn't initialize the variables yet.\n",
        "ugrnn_model = RNNModel(vocabulary_size=len(word2idx), rnn_cell='ugrnn', \n",
        "                       device=device, checkpoint_directory=checkpoint_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoVu6gXX-h4W"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "ugrnn_model.fit(train_dataset, test_dataset, optimizer, num_epochs=50, \n",
        "                early_stopping_rounds=5, verbose=1, train_from_scratch=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yH1YtzN_DPO"
      },
      "source": [
        "Performance comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JByyw_mo-kJB"
      },
      "outputs": [],
      "source": [
        "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 4))\n",
        "ax1.plot(range(len(lstm_model.history['train_acc'])), lstm_model.history['train_acc'], \n",
        "         label='LSTM Train Accuracy');\n",
        "ax1.plot(range(len(lstm_model.history['eval_acc'])), lstm_model.history['eval_acc'], \n",
        "         label='LSTM Test Accuracy');\n",
        "ax2.plot(range(len(ugrnn_model.history['train_acc'])), ugrnn_model.history['train_acc'],\n",
        "         label='UGRNN Train Accuracy');\n",
        "ax2.plot(range(len(ugrnn_model.history['eval_acc'])), ugrnn_model.history['eval_acc'],\n",
        "         label='UGRNN Test Accuracy');\n",
        "ax1.legend();\n",
        "ax2.legend();"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "597_RNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPPjCTfZxbLQuCs1FC6k43c",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}