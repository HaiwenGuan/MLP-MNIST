{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaiwenGuan/MLP-MNIST/blob/main/MLP_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THehMjIEXR-L"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "843yd00FXRVR",
        "outputId": "c526262d-c7d8-426a-e952-3cf4d3203180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from keras.datasets import mnist\n",
        "from keras.datasets import fashion_mnist\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "np.random.seed(4208)\n",
        "tf.random.set_seed(4208)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "C2fSADF8G1yO",
        "outputId": "d25f3a2e-9b42-4fce-f5f7-a038b7b1ba70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "tf.executing_eagerly()\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-UeIfWBXkxk",
        "outputId": "ae2a69d5-4bcb-4220-991f-9d7176c6906c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "tf.config.list_physical_devices('CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djjVvFhjXuYM"
      },
      "source": [
        "Generate random data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "E6V-x93MXsjy"
      },
      "outputs": [],
      "source": [
        "size_input = 784\n",
        "size_hidden_1 = 256\n",
        "size_hidden_2 = 128\n",
        "size_output = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "BngVCmxgX8pJ"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
        "X_train = tf.cast(tf.reshape(X_train, (-1, 784)), dtype=tf.float32)\n",
        "X_test = tf.cast(tf.reshape(X_test, (-1, 784)), dtype=tf.float32)\n",
        "y_train = tf.one_hot(y_train, 10)\n",
        "y_test = tf.one_hot(y_test, 10)\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Q7faEczYYpjf"
      },
      "outputs": [],
      "source": [
        "#split dataset into batches\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(100)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRAaj0uAY_1T"
      },
      "source": [
        "Build MLP using Eager Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "n5D06pB5Y_Xd"
      },
      "outputs": [],
      "source": [
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden_1, size_hidden_2, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: etiher 'cpu' or 'gpu' or None. If None, decided automatically during eager.\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden_1, self.size_hidden_2, self.size_output, self.device =\\\n",
        "    size_input, size_hidden_1, size_hidden_2, size_output, device\n",
        "\n",
        "    #Initialize weights between input layer and hidden layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden_1]))\n",
        "    self.b1 = tf.Variable(tf.random.normal([1,self.size_hidden_1]))\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden_1, self.size_hidden_2]))\n",
        "    self.b2 = tf.Variable(tf.random.normal([1,self.size_hidden_2]))\n",
        "    self.Wop = tf.Variable(tf.random.normal([self.size_hidden_2, self.size_output]))\n",
        "    self.bop = tf.Variable(tf.random.normal([1,self.size_output]))\n",
        "\n",
        "    self.variables = [self.W1, self.W2, self.b1, self.b2, self.Wop, self.bop]  # define variables(parameters) that will be updated \n",
        "\n",
        "  def forward(self, X): # (X is the input matrix)\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "    \n",
        "  def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape(batch_size, size_output)\n",
        "    y_true - Tensor of shape(batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    return cce(y_true, y_pred)\n",
        "\n",
        "  def loss_l1(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape(batch_size, size_output)\n",
        "    y_true - Tensor of shape(batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    penalty = (tf.reduce_sum(tf.math.abs(self.W1)).numpy() + tf.reduce_sum(tf.math.abs(self.W2)).numpy() + tf.reduce_sum(tf.math.abs(self.Wop).numpy())) / ( tf.size(self.W1).numpy() + tf.size(self.W2).numpy() + tf.size(self.Wop).numpy() )\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    return cce(y_true, y_pred) + penalty\n",
        "\n",
        "  def loss_l2(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape(batch_size, size_output)\n",
        "    y_true - Tensor of shape(batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    penalty = (tf.reduce_sum(tf.math.square(self.W1)).numpy() + tf.reduce_sum(tf.math.square(self.W2)).numpy() + tf.reduce_sum(tf.math.square(self.Wop)).numpy())/ ( tf.size(self.W1).numpy() + tf.size(self.W2).numpy() + tf.size(self.Wop).numpy() )\n",
        "    #penalty = self.l2 * (tf.reduce_sum(tf.math.square(self.W1)).numpy() + tf.reduce_sum(tf.math.square(self.W2)).numpy() + tf.reduce_sum(tf.math.square(self.Wop).numpy()))\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    return cce(y_true, y_pred) + penalty\n",
        "\n",
        "  def backward(self, X_train, y_train):\n",
        "    '''\n",
        "    backward pass\n",
        "    '''\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients( zip( grads, self.variables))\n",
        "\n",
        "  def backward_1(self, X_train, y_train):\n",
        "    '''\n",
        "    backward pass\n",
        "    '''\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss_l1(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients( zip( grads, self.variables))\n",
        "\n",
        "  def backward_2(self, X_train, y_train):\n",
        "    '''\n",
        "    backward pass\n",
        "    '''\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss_l2(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients( zip( grads, self.variables))\n",
        "    \n",
        "  def compute_output(self, X):\n",
        "#    X_tf = tf.cast(X, dytpe=tf.float32)\n",
        "    what1 = tf.matmul(X, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "#    output = tf.nn.softmax(tf.matmul(hhat2, self.Wop) + self.bop)\n",
        "    output = tf.matmul(hhat2, self.Wop) + self.bop\n",
        "\n",
        "    return output\n",
        "\n",
        "  def compute_output(self, X):\n",
        "#    X_tf = tf.cast(X, dytpe=tf.float32)\n",
        "    what1 = tf.matmul(X, self.W1) + self.b1\n",
        "    hhat1 = tf.keras.activations.relu(what1)\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.keras.activations.relu(what2)\n",
        "#    output = tf.nn.softmax(tf.matmul(hhat2, self.Wop) + self.bop)\n",
        "    output = tf.matmul(hhat2, self.Wop) + self.bop\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvTC-aKeANc2"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Ims8PMhtAK7h"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "aw5bbqk_AR7v",
        "outputId": "72d784e0-495e-4ee0-e319-babf442eee95"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-86718eecd95b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_on_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_total\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmlp_on_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_l2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mlt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmlp_on_gpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-fc194fe335eb>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0my_pred_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mcce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mloss_l1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m       return losses_utils.compute_weighted_loss(\n\u001b[0;32m--> 143\u001b[0;31m           losses, sample_weight, reduction=self._get_reduction())\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/losses_utils.py\u001b[0m in \u001b[0;36mcompute_weighted_loss\u001b[0;34m(losses, sample_weight, reduction, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;31m# Apply reduction function to the individual weighted losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_weighted_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m     \u001b[0;31m# Convert the result back to the input type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/losses_utils.py\u001b[0m in \u001b[0;36mreduce_weighted_loss\u001b[0;34m(weighted_losses, reduction)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mReductionV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUM_OVER_BATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_num_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/losses_utils.py\u001b[0m in \u001b[0;36m_num_elements\u001b[0;34m(losses)\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;34m\"\"\"Computes the number of elements in `losses` tensor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'num_elements'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msize_v2\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m    732\u001b[0m   \"\"\"\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msize\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    764\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m   \"\"\"\n\u001b[0;32m--> 766\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msize_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36msize_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0mnp_out_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m     \u001b[0mnum_elements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_out_type\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_elements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Initialize model using CPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden_1, size_hidden_2, size_output, device='cpu')\n",
        "m = tf.keras.metrics.CategoricalAccuracy()\n",
        "time_start = time.time()\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1],dtype=tf.float32)\n",
        "  lt = 0\n",
        "  accuracy = []\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(100, seed=epoch*(4209)).batch(100)\n",
        "  for inputs, outputs in train_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_gpu.loss_l2(preds, outputs)\n",
        "    lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "    m.update_state(outputs,preds)\n",
        "    accuracy.append(m.result().numpy())\n",
        "    mlp_on_gpu.backward_2(inputs, outputs)\n",
        "  #print('Number of Epoch = {} - Average Loss:= {}'.format(epoch + 1, np.sum(loss_total) / 600))\n",
        "  m.reset_state()\n",
        "  print('Number of Epoch = {} - Accuracy:= {} - Average Loss:= {}'.format(epoch + 1, np.sum(accuracy)/len(accuracy), np.sum(loss_total)/X_train.shape[0]))\n",
        "time_taken = time.time() - time_start\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMJvHryKXpdr"
      },
      "outputs": [],
      "source": [
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLJG4b_fw_jL",
        "outputId": "6d1786b4-58a2-43a4-c6c0-7f826bafeca2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9172 - Loss: 0.0973\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0,dtype=tf.float32)\n",
        "accuracy = []\n",
        "m = tf.keras.metrics.CategoricalAccuracy()\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_gpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_gpu.loss_l2(preds, outputs)\n",
        "  m.update_state(outputs,preds)\n",
        "  accuracy.append(m.result().numpy())\n",
        "#print('Test MSE: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_train.shape[0]))\n",
        "print('Accuracy: {:.4f} - Loss: {:.4f}'.format(np.sum(accuracy) / len(accuracy), np.sum(test_loss_total.numpy()) / X_train.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgA0ST9xiV77"
      },
      "outputs": [],
      "source": [
        "seed = 4208\n",
        "#f = open(\"/content/drive/My Drive/Colab Notebooks/MLP/MNIST_result.txt\",\"w\")\n",
        "lines = []\n",
        "time_start = time.time()\n",
        "for i in range(10):\n",
        "  print(\"==============\" + str(i)+\"=============\")\n",
        "  seed = seed + i\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  tf.random.set_seed(seed)\n",
        "\n",
        "  mlp_on_gpu = MLP(size_input, size_hidden_1, size_hidden_2, size_output, device='cpu')\n",
        "  m = tf.keras.metrics.CategoricalAccuracy()\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    loss_total = tf.zeros([1,1],dtype=tf.float32)\n",
        "    lt = 0\n",
        "    accuracy = []\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(100, seed=epoch*(4209)).batch(100)\n",
        "    for inputs, outputs in train_ds:\n",
        "      preds = mlp_on_gpu.forward(inputs)\n",
        "      loss_total = loss_total + mlp_on_gpu.loss_l2(preds, outputs)\n",
        "      lt = lt + mlp_on_gpu.loss(preds, outputs)\n",
        "      m.update_state(outputs,preds)\n",
        "      accuracy.append(m.result().numpy())\n",
        "      mlp_on_gpu.backward_2(inputs, outputs)\n",
        "    #print('Number of Epoch = {} - Average Loss:= {}'.format(epoch + 1, np.sum(loss_total) / 600))\n",
        "    m.reset_state()\n",
        "    print('Number of Epoch = {} - Accuracy:= {} - Average Loss:= {}\\n'.format(epoch + 1, np.sum(accuracy)/len(accuracy), np.sum(loss_total)/X_train.shape[0]))\n",
        "    lines.append('Number of Epoch = {} - Accuracy:= {} - Average Loss:= {}\\n'.format(epoch + 1, np.sum(accuracy)/len(accuracy), np.sum(loss_total)/X_train.shape[0]))\n",
        "  test_loss_total = tf.Variable(0,dtype=tf.float32)\n",
        "  accuracy = []\n",
        "  n = tf.keras.metrics.CategoricalAccuracy()\n",
        "  for inputs, outputs in test_ds:\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    test_loss_total = test_loss_total + mlp_on_gpu.loss_l2(preds, outputs)\n",
        "    n.update_state(outputs,preds)\n",
        "    accuracy.append(n.result().numpy())\n",
        "#print('Test MSE: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_train.shape[0]))\n",
        "  lines.append('Test Accuracy: {:.4f} - Loss: {:.4f}\\n'.format(np.sum(accuracy) / len(accuracy), np.sum(test_loss_total.numpy()) / X_train.shape[0]))\n",
        "  print('Test Accuracy: {:.4f} - Loss: {:.4f}\\n'.format(np.sum(accuracy) / len(accuracy), np.sum(test_loss_total.numpy()) / X_train.shape[0]))\n",
        "time_taken = time.time() - time_start\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#f.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/My Drive/Colab Notebooks/MLP/MNIST_result.txt\",\"w\") as f:\n",
        "  for i in lines:\n",
        "    f.write(i)\n"
      ],
      "metadata": {
        "id": "5_N19eCsS4O_"
      },
      "execution_count": 58,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MLP_MNIST.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO0Cb7R/A5i41WMroJeQHrr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}